(csce) C:\Users\hussein\Desktop\DL-GPT-for-scan>python main.py --task train --n_layer 8 --n_head 8 --n_embd 256 --max_epochs 60 --batch_size 64 
GPU is enabled.
The file './tokenizer/simple_vocab.json' exists. Loading tokenizer.
{'<pad>': 0, '<s>': 1, '</s>': 2, '<unk>': 3, 'I_TURN_RIGHT': 4, 'I_JUMP': 5, 'I_WALK': 6, 'I_TURN_LEFT': 7, 'I_RUN': 8, 'I_LOOK': 9, 'jump': 10, 'opposite': 11, 'right': 12, 'twice': 13, 'and': 14, 'turn': 15, 'thrice': 16, 'run': 17, 'left': 18, 'after': 19, 'walk': 20, 'around': 21, 'look': 22}
train dataset size: 15055
val dataset size: 1673
loading model
total params: 6343168
epoch 1 iter 235: train loss 0.47933. lr 3.9979e-04: 100%|████████████████████████████████████████████████████████████████| 236/236 [00:12<00:00, 18.76it/s]
test loss: %f 0.48948601329768143
epoch 2 iter 235: train loss 0.21911. lr 3.9905e-04: 100%|████████████████████████████████████████████████████████████████| 236/236 [00:12<00:00, 19.29it/s]
test loss: %f 0.25213630221508165
step_train_loss: 0.24426554143428802 train_step: 500, learning_rate: 0.00039892361395936477                                | 26/236 [00:01<00:09, 23.13it/s]
epoch 3 iter 235: train loss 0.16688. lr 3.9779e-04: 100%|████████████████████████████████████████████████████████████████| 236/236 [00:13<00:00, 17.71it/s]
test loss: %f 0.1271039581409207
epoch 4 iter 235: train loss 0.25259. lr 3.9601e-04: 100%|████████████████████████████████████████████████████████████████| 236/236 [00:12<00:00, 18.72it/s]
test loss: %f 0.10020589042041037
step_train_loss: 0.15803417563438416 train_step: 1000, learning_rate: 0.000395496378199383                                 | 54/236 [00:02<00:08, 22.67it/s]
epoch 5 iter 235: train loss 0.10325. lr 3.9371e-04: 100%|████████████████████████████████████████████████████████████████| 236/236 [00:12<00:00, 19.54it/s]
test loss: %f 0.07183137563643632
epoch 6 iter 235: train loss 0.14093. lr 3.9090e-04: 100%|████████████████████████████████████████████████████████████████| 236/236 [00:13<00:00, 17.61it/s]
test loss: %f 0.055148416509230934
step_train_loss: 0.09699562191963196 train_step: 1500, learning_rate: 0.00038975631233084646                               | 83/236 [00:05<00:09, 15.87it/s]
epoch 7 iter 235: train loss 0.07673. lr 3.8758e-04: 100%|████████████████████████████████████████████████████████████████| 236/236 [00:12<00:00, 18.42it/s]
test loss: %f 0.043730592010197814
epoch 8 iter 235: train loss 0.10287. lr 3.8377e-04: 100%|████████████████████████████████████████████████████████████████| 236/236 [00:12<00:00, 19.27it/s]
test loss: %f 0.06005552355889921
step_train_loss: 0.06750908493995667 train_step: 2000, learning_rate: 0.0003817713245026842                               | 111/236 [00:05<00:07, 15.77it/s]
epoch 9 iter 235: train loss 0.12358. lr 3.7948e-04: 100%|████████████████████████████████████████████████████████████████| 236/236 [00:13<00:00, 17.29it/s] 
test loss: %f 0.03127420493574054
epoch 10 iter 235: train loss 0.05305. lr 3.7471e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:12<00:00, 18.53it/s]
test loss: %f 0.036275912904077105
step_train_loss: 0.07373650372028351 train_step: 2500, learning_rate: 0.000371635881530269██████                          | 139/236 [00:06<00:04, 22.88it/s]
epoch 11 iter 235: train loss 0.06616. lr 3.6949e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:12<00:00, 19.60it/s] 
test loss: %f 0.04201269584397475
epoch 12 iter 235: train loss 0.03203. lr 3.6381e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:13<00:00, 17.60it/s]
test loss: %f 0.026058283117082384
step_train_loss: 0.0534062534570694 train_step: 3000, learning_rate: 0.00035946989130082645████████████▊                  | 168/236 [00:10<00:03, 21.45it/s]
epoch 13 iter 235: train loss 0.04496. lr 3.5771e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:13<00:00, 17.23it/s] 
test loss: %f 0.028775846447657655
epoch 14 iter 235: train loss 0.02654. lr 3.5119e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:12<00:00, 19.44it/s]
test loss: %f 0.015329749564881678
step_train_loss: 0.05265166983008385 train_step: 3500, learning_rate: 0.000345417284196915█████████████████████▎          | 196/236 [00:10<00:02, 16.01it/s]
epoch 15 iter 235: train loss 0.09287. lr 3.4427e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:13<00:00, 17.97it/s] 
test loss: %f 0.015884558256301615
epoch 16 iter 235: train loss 0.01953. lr 3.3697e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:13<00:00, 17.96it/s]
test loss: %f 0.02233316835567907
step_train_loss: 0.033748649060726166 train_step: 4000, learning_rate: 0.0003296443103205211██████████████████████████▎   | 222/236 [00:10<00:00, 23.63it/s]
epoch 17 iter 235: train loss 0.05137. lr 3.2930e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:11<00:00, 19.99it/s] 
test loss: %f 0.01608428424569192
epoch 18 iter 235: train loss 0.00850. lr 3.2130e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:21<00:00, 10.76it/s]
test loss: %f 0.01617287620212193
epoch 19 iter 235: train loss 0.07089. lr 3.1298e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:20<00:00, 11.68it/s]
test loss: %f 0.008286032773968246
step_train_loss: 0.04409375786781311 train_step: 4500, learning_rate: 0.000312365228652508                                 | 16/236 [00:01<00:21, 10.29it/s]
epoch 20 iter 235: train loss 0.00571. lr 3.0436e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:16<00:00, 14.18it/s] 
test loss: %f 0.008722936260272507
epoch 21 iter 235: train loss 0.01018. lr 2.9546e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:20<00:00, 11.54it/s]
test loss: %f 0.00480772207784294
step_train_loss: 0.005127156153321266 train_step: 5000, learning_rate: 0.000293731351426208                                | 44/236 [00:04<00:21,  9.05it/s]
epoch 22 iter 235: train loss 0.01162. lr 2.8632e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:25<00:00,  9.31it/s] 
test loss: %f 0.006287593793348168
epoch 23 iter 235: train loss 0.02603. lr 2.7694e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:34<00:00,  6.82it/s]
test loss: %f 0.0028684424727948177
step_train_loss: 0.007710054516792297 train_step: 5500, learning_rate: 0.00027398858055343347                              | 72/236 [00:08<00:20,  7.91it/s]
epoch 24 iter 235: train loss 0.00512. lr 2.6736e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:32<00:00,  7.19it/s] 
test loss: %f 0.008958564579693807
epoch 25 iter 235: train loss 0.01685. lr 2.5761e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:34<00:00,  6.93it/s]
test loss: %f 0.0040965106627145025
step_train_loss: 0.006851965561509132 train_step: 6000, learning_rate: 0.0002533704979048041                              | 100/236 [00:11<00:15,  8.52it/s]
epoch 26 iter 235: train loss 0.02294. lr 2.4770e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:31<00:00,  7.43it/s] 
test loss: %f 0.0031947620222194085
epoch 27 iter 235: train loss 0.02218. lr 2.3767e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:29<00:00,  7.99it/s]
test loss: %f 0.002133542466994927
step_train_loss: 0.01277198176831007 train_step: 6500, learning_rate: 0.0002321209706546617██▏                            | 128/236 [00:17<00:12,  8.88it/s]
epoch 28 iter 235: train loss 0.01096. lr 2.2753e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:33<00:00,  7.07it/s] 
test loss: %f 0.0019556094641167737
epoch 29 iter 235: train loss 0.00694. lr 2.1733e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:32<00:00,  7.29it/s]
test loss: %f 0.0068653176928853135
step_train_loss: 0.020441288128495216 train_step: 7000, learning_rate: 0.0002104914915534168████████▋                     | 156/236 [00:12<00:08,  9.73it/s]
epoch 30 iter 235: train loss 0.01177. lr 2.0708e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:22<00:00, 10.70it/s] 
test loss: %f 0.0030058290366367954
epoch 31 iter 235: train loss 0.01064. lr 1.9681e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:27<00:00,  8.47it/s]
test loss: %f 0.0014429873632185195
step_train_loss: 0.00692693330347538 train_step: 7500, learning_rate: 0.0001887378346746685█████████████████              | 184/236 [00:27<00:07,  6.78it/s]
epoch 32 iter 235: train loss 0.00059. lr 1.8655e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:35<00:00,  6.65it/s] 
test loss: %f 0.0027076598624397002
epoch 33 iter 235: train loss 0.00879. lr 1.7632e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:32<00:00,  7.22it/s]
test loss: %f 0.0012970123926707958
step_train_loss: 0.014619608409702778 train_step: 8000, learning_rate: 0.0001671174730576032███████████████████████▌      | 212/236 [00:18<00:03,  6.82it/s]
epoch 34 iter 235: train loss 0.00746. lr 1.6616e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:22<00:00, 10.58it/s] 
test loss: %f 0.0011030410291655076
epoch 35 iter 235: train loss 0.00047. lr 1.5609e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:25<00:00,  9.23it/s]
test loss: %f 0.00046789646916699386
epoch 36 iter 235: train loss 0.00041. lr 1.4613e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:30<00:00,  7.68it/s]
test loss: %f 0.000799605983990693
step_train_loss: 0.006520713679492474 train_step: 8500, learning_rate: 0.0001459182699202786                                | 4/236 [00:00<00:28,  8.08it/s]
epoch 37 iter 235: train loss 0.00654. lr 1.3631e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:32<00:00,  7.33it/s] 
test loss: %f 0.0013253088002750668
epoch 38 iter 235: train loss 0.00038. lr 1.2666e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:32<00:00,  7.21it/s]
test loss: %f 0.0008840098795459468
step_train_loss: 0.0028607400599867105 train_step: 9000, learning_rate: 0.00012532592316522907                             | 32/236 [00:05<00:32,  6.24it/s]
epoch 39 iter 235: train loss 0.00081. lr 1.1721e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:33<00:00,  6.99it/s] 
test loss: %f 0.0015188666918590941
epoch 40 iter 235: train loss 0.00025. lr 1.0797e-04: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:33<00:00,  6.95it/s]
test loss: %f 0.00022260445122676038
step_train_loss: 0.00040291232289746404 train_step: 9500, learning_rate: 0.00010561701197467799                            | 60/236 [00:07<00:18,  9.28it/s]
epoch 41 iter 235: train loss 0.00008. lr 9.8980e-05: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:35<00:00,  6.68it/s] 
test loss: %f 0.0011168219666446762
epoch 42 iter 235: train loss 0.00014. lr 9.0252e-05: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:31<00:00,  7.39it/s]
test loss: %f 0.00039677087355543064
step_train_loss: 0.0007685605087317526 train_step: 10000, learning_rate: 8.702470365140719e-05                             | 87/236 [00:10<00:10, 14.53it/s]
epoch 43 iter 235: train loss 0.00401. lr 8.1814e-05: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:25<00:00,  9.24it/s] 
test loss: %f 0.0006126557275112848
epoch 44 iter 235: train loss 0.00060. lr 7.3688e-05: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:32<00:00,  7.23it/s]
test loss: %f 0.0003830717554659781
step_train_loss: 0.004184646997600794 train_step: 10500, learning_rate: 6.976895546968239e-05                             | 116/236 [00:18<00:19,  6.29it/s]
epoch 45 iter 235: train loss 0.00004. lr 6.5895e-05: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:37<00:00,  6.22it/s] 
test loss: %f 0.0007839267334390941
epoch 46 iter 235: train loss 0.00401. lr 5.8455e-05: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:36<00:00,  6.50it/s]
test loss: %f 0.00012481144363442177
step_train_loss: 0.0027084206230938435 train_step: 11000, learning_rate: 5.405391245923741e-05███▍                        | 144/236 [00:14<00:14,  6.33it/s]
epoch 47 iter 235: train loss 0.00015. lr 5.1389e-05: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:29<00:00,  8.07it/s] 
test loss: %f 0.0001604425692844584
epoch 48 iter 235: train loss 0.01281. lr 4.4714e-05: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:29<00:00,  8.13it/s]
test loss: %f 0.00022373931133862007
step_train_loss: 6.866110197734088e-05 train_step: 11500, learning_rate: 4.006549225684151e-05██████████▋                 | 171/236 [00:22<00:06,  9.92it/s]
epoch 49 iter 235: train loss 0.01843. lr 4.0000e-05: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:30<00:00,  7.73it/s] 
test loss: %f 5.8636101938544824e-05
epoch 50 iter 235: train loss 0.00014. lr 4.0000e-05: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:30<00:00,  7.79it/s]
test loss: %f 0.00016447377533444928
step_train_loss: 0.00022294236987363547 train_step: 12000, learning_rate: 4e-05█████████████████████████████████▍         | 200/236 [00:18<00:03, 10.16it/s]
epoch 51 iter 235: train loss 0.00084. lr 4.0000e-05: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:23<00:00, 10.15it/s] 
test loss: %f 1.6863336963137112e-05
epoch 52 iter 235: train loss 0.00109. lr 4.0000e-05: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:29<00:00,  8.06it/s]
test loss: %f 4.769409901604781e-05
step_train_loss: 0.0005613320390693843 train_step: 12500, learning_rate: 4e-05█████████████████████████████████████████▊  | 228/236 [00:33<00:01,  6.76it/s]
epoch 53 iter 235: train loss 0.00005. lr 4.0000e-05: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:35<00:00,  6.67it/s] 
test loss: %f 3.378368563533985e-05
epoch 54 iter 235: train loss 0.00003. lr 4.0000e-05: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:30<00:00,  7.63it/s]
test loss: %f 3.788758084300315e-05
epoch 55 iter 235: train loss 0.00003. lr 4.0000e-05: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:22<00:00, 10.63it/s]
test loss: %f 0.00011631123972485377
step_train_loss: 0.0019333752570673823 train_step: 13000, learning_rate: 4e-05                                             | 20/236 [00:03<00:31,  6.77it/s]
epoch 56 iter 235: train loss 0.00009. lr 4.0000e-05: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:28<00:00,  8.38it/s] 
test loss: %f 1.6429858684811432e-05
epoch 57 iter 235: train loss 0.00006. lr 4.0000e-05: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:30<00:00,  7.70it/s]
test loss: %f 5.05153287930524e-05
step_train_loss: 0.001868042629212141 train_step: 13500, learning_rate: 4e-05                                              | 48/236 [00:05<00:18,  9.94it/s]
epoch 58 iter 235: train loss 0.00002. lr 4.0000e-05: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:27<00:00,  8.50it/s] 
test loss: %f 4.127424030971101e-05
epoch 59 iter 235: train loss 0.00183. lr 4.0000e-05: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:26<00:00,  8.83it/s]
test loss: %f 1.5322521701616803e-05
step_train_loss: 5.7144410675391555e-05 train_step: 14000, learning_rate: 4e-05                                            | 76/236 [00:11<00:23,  6.77it/s]
epoch 60 iter 235: train loss 0.00079. lr 4.0000e-05: 100%|███████████████████████████████████████████████████████████████| 236/236 [00:31<00:00,  7.45it/s] 
test loss: %f 2.7638378570473935e-05
epoch_valid_loss: 2.7638378570473935e-05, epoch_train_loss: 0.0010025884873079954, epoch: 60
Saving at epoch 60: ./cond_gpt/weights/None_simplesplit_8layer_8head_256embd_64bs.pt